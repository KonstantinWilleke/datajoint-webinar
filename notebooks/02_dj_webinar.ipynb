{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome back to the second DataJoint webinar!\n",
    "\n",
    "This webinar builds on the mouse electrophysiology pipeline we started building in the first webinar. By the end of this webinar you will have learnt how to...\n",
    "\n",
    "* ...import neuron activity data from files into an `Imported` table\n",
    "* ...compute various statistics for each neuron using a `Computed` table\n",
    "* ...store computation parameters in a `Lookup` table\n",
    "* ...perform spike detection using another `Computed` table\n",
    "* ...trigger automatic computations for all missing entries in the aforementioned tables using the `populate` method\n",
    "\n",
    "First of all let us import DataJoint again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need NumPy to perform the computations and Matplotlib to visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore we need the previously defined `Mouse` and `Session` tables or more specifically their corresponding Python classes. These are conveniently provided in the `session02` module of the `webinar` package from which we can import them. We also want to import the `schema` object to be able to define additional tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webinar.session02 import Mouse, Session, schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the ERD of the schema to remind ourselves what we have accomplished in the previous webinar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mouse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tables live in a new schema different from the one you used in the first webinar and they are already prefilled with data to ensure that we are all on the same page.\n",
    "\n",
    "## Exercise: Recapping the first webinar\n",
    "\n",
    "Insert a female mouse with ID 42 that was born on the 12th of May 2018 into the `Mouse` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the start and end times of the session performed on the mouse with ID 0 on the 15th of May 2017 from the `Session` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the sex of all mice that had sessions performed on them as a list of dictionaries from the `Mouse` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the mouse with ID 42 from the `Mouse` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data from files\n",
    "\n",
    "Recall from the project description\n",
    "> * In each experimental session you record electrical activity from a single **neuron** and you use recording equipment that produces separate data files for each recorded neuron.\n",
    "\n",
    "Because we record from a single neuron in each session and the data from each recorded neuron is stored in a single file we should have a single file per recording session.\n",
    "\n",
    "You can find these files in the `../data` directory. They contain saved Numpy arrays as indicated by their `.npy` extension and their names contain the mouse ID and the recording session date in the format `data_{mouse_id}_{session_date}.npy`. For example the file with name `data_100_2017-05-25.npy` contains data from the recording session perfomed on the 25th of May 2017 on the mouse with ID 100.\n",
    "\n",
    "### Looking at the data\n",
    "\n",
    "Let's take a quick look at the contents of the data files to get a better understandin of what we will be working with.\n",
    "\n",
    "First we fetch the primary keys of all recording sessions by passing the special `\"KEY\"` argument to the `fetch` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = Session.fetch(\"KEY\")\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that each key in the resulting list can be used to uniquely identify a particular recording session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR CODE! - restrict the session table with a single key of your choosing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now let us select the first key to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = keys[0]\n",
    "key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to create the name of the file corresponding to the selected key. Remember that we need the `mouse_id` and the `session_date` to create the names of our data files and conveniently precisely this information is contained in the key we just selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/data_{mouse_id}_{session_date}.npy\".format(**key)\n",
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used Python's dictionary unpacking and the `format` method to plug the correct values into our filename convention.\n",
    "\n",
    "Finally let's load the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at its content..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and check the shape of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the file contains a NumPy array of with 1000 entries. This represents a (simulated) recording of raw electric activity from a single neuron over 1000 time bins.\n",
    "\n",
    "### Defining the Neuron table\n",
    "\n",
    "Now we would like to have all these recorded neurons represented in the pipeline and store their activity alongside them. This will allow us to perform computations in the pipeline using the stored data.\n",
    "\n",
    "Remember that we record from a single neuron in each session and therefore each neuron can be uniquely identified by a particular session. This in turn means that our `Neuron` table should depend on the already existing `Session` table. Furthermore we want to include a non-primary key attribute which will contain the neuron's recoreded activity imported from the respective data file. \n",
    "\n",
    "Let's define the `Neuron` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class Neuron(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> Session\n",
    "    ---\n",
    "    activity: longblob  # electric neuron activity\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used the `longblob` datatype to store the activity of our neurons. This datatype can be used to store arbitrary numeric arrays like for example our NumPy arrays which will be imported from the data file corresponding to each neuron. Furthermore we used the `dj.Imported` class as the base class for our `Neuron` table instead of the previously used `dj.Manual` class. This indicates that the contents of the table will depend on data imported from external files.\n",
    "\n",
    "Let's check the state of our pipeline after the creation of the `Neuron` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR CODE! - plot the ERD of the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our new `Neuron` table is represented by a blue ellipse, nicely distinguishing itself from the previously defined manual tables represented by green rectangles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataJoint data tiers\n",
    "\n",
    "Each table in DataJoint belongs to a certain data tier. The data tier of a particular table indicates the **nature of the data it contains and the source of that data**. So far we have been introduced to two data tiers: Manual and Imported. We will encounter the two other major data tiers shortly.\n",
    "\n",
    "DataJoint tables belonging to the Manual data tier, or simply **Manual tables**, indicate that their contents are **manually** entered by either the experimenter or some system external to DataJoint and their contents **do not depend on other tables or external files**. The Manual data tier is the most basic data tier you will encounter and especially tables at the beginning of a pipeline will belong to that tier.\n",
    "\n",
    "**Imported tables** on the other hand are understood to import their contents from external files and they come equipped with functionality to perform this importing process automatically, as we will see momentarily. \n",
    "\n",
    "### Importing data into the Imported table\n",
    "\n",
    "Rather than manually adding entries to the `Neuron` table using the `insert1` or `insert` methods, we will make use of the `make` and `populate` methods of imported tables to automate the importing process.\n",
    "\n",
    "### make and populate methods\n",
    "\n",
    "Imported tables come with a special method called `populate`. Let's try calling it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR CODE! - call the populate method of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `populate` call produced an error complaining about a method called `make` not being implemented. Let's implement a simple `make` method to elucidate what this is all about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class Neuron(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> Session\n",
    "    ---\n",
    "    activity: longblob # electric neuron activity\n",
    "    \"\"\"\n",
    "\n",
    "    def make(self, key):  # The 'make' method takes a single argument called 'key'\n",
    "        print(\"Key is\", key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to populate the table again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR CODE! - call the populate method of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the `populate` method on a imported table triggers DataJoint to look up all tables that the imported table depends on (i.e. its \"parent\" tables).\n",
    "\n",
    "DataJoint will call the `make` function for each **unique combination of entries in the \"parent\" tables** passing in the primary key of the parent(s).\n",
    "\n",
    "Because the `Neuron` table depends on the `Session` table, `Neuron`'s `make` method was called for each entry in `Session` with the respective primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `make` method only received the primary key attributes of the `Session` table and not the other non-primary ones.\n",
    "\n",
    "### Implementing the make method\n",
    "\n",
    "Now that we have a better understanding of how the `make` method works, let's implement it properly so that it can import the data from the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class Neuron(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> Session\n",
    "    ---\n",
    "    activity: longblob # electric neuron activity\n",
    "    \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "        # Create data file path from key\n",
    "        data_filepath = \"../data/data_{mouse_id}_{session_date}.npy\".format(**key)\n",
    "\n",
    "        # Load data from created file path\n",
    "        data = np.load(data_filepath)\n",
    "\n",
    "        # Add the loaded data to the key in the \"activity\" column\n",
    "        key[\"activity\"] = data\n",
    "\n",
    "        # Insert the key into self\n",
    "        self.insert1(key)\n",
    "\n",
    "        print(\"Populated a neuron for mouse_id={mouse_id} and session_date={session_date}\".format(**key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we add the loaded data to the key dictionary under the `\"activity\"` column. Afterwards we simply insert the key into `self` (i.e. into the `Neuron` table) because now the key contains all the information needed for a valid entry in the `Neuron` table. The whole job of the `make` method is to create a new entry based on the key it received and insert the new entry into itself.\n",
    "\n",
    "Finally let's call `populate` once more to populate the `Neuron` table with entries, filling it with data loaded from the data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neuron.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neuron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we call `populate` again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neuron.populate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's right - nothing! This makes sense because we already populated the `Neuron` table with all the sessions currently in our `Session` table. There is currently nothing more to import.\n",
    "\n",
    "Let's change that by adding a new session to the `Session` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Session.insert1(\n",
    "    {\n",
    "        \"mouse_id\": 100,\n",
    "        \"session_date\": \"2017-06-01\",\n",
    "        \"experiment_setup\": 1,\n",
    "        \"experimenter\": \"Jacob Reimer\",\n",
    "        \"start\": \"09:52:14\",\n",
    "        \"end\": \"11:19:53\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list all entries in the `Session` table that have no corresponding entry in the `Neuron` table by using the **exclusion operator** `-`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Session - Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens now that we have a new entry in the `Session` table when we populate the `Neuron` table again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neuron.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neuron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the call to `populate` triggered the `make` method of the `Neuron` table which created a new entry corresponding to the newly added session.\n",
    "\n",
    "## Computations in the data pipeline\n",
    "\n",
    "Now that we have imported all our data into the pipeline we can start analyzing it.\n",
    "\n",
    "When performing computations in the DataJoint pipeline we want to think about **what** we want to compute and not how we want to compute it. Once we have a good idea about the \"what\" we can design tables representing it.\n",
    "\n",
    "Let's say the \"what\" in our current pipeline are **activity statistics**, i.e. the mean, standard deviation and maximum of our previously imported neuronal activitiy traces. \n",
    "\n",
    "Therefore we need a new table representing the activity statistics of a neuron. Let's start designing that table paying special attention to its dependencies. \n",
    "\n",
    "### Neuron activity statistics\n",
    "\n",
    "Before we create the table to store the computed statistics, let's think about how we might go about computing the statistics for a single neuron.\n",
    "\n",
    "Let's start by fetching one neuron to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = Neuron().fetch(\"KEY\")\n",
    "key = keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neuron() & key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's grab the activity data stored as a NumPy array. From the last session we know that we can simply fetch it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = (Neuron & key).fetch(\"activity\")\n",
    "activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `fetch` method wraps the fetched data in a NumPy array. This is the case even if the data itself is a NumPy array to begin with. So here we actually got a NumPy array of a NumPy array. That said we can simply index into the outer array and retrieve our activity data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can get around this nested array issue if we simply use the `insert1` instead of the `insert` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = (Neuron & key).fetch1(\"activity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR CODE! - compute the mean activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR CODE! - compute the standard deviation of the activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR CODE! - compute the maximum activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a good idea on how to:\n",
    "\n",
    "1. Fetch the activity data of a neuron based on its primary key\n",
    "2. Compute statistics on the fetched data\n",
    "\n",
    "Armed with this knowledge let's go ahead and define the `ActivityStatistics` table!\n",
    "\n",
    "### Defining the ActivityStatistics table\n",
    "\n",
    "Let's start by working out the definition of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class ActivityStatistics(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> Neuron\n",
    "    ---\n",
    "    mean: float  # mean activity\n",
    "    stdev: float  # standard deviation of activity\n",
    "    max: float  # maximum activity\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that we are now inheriting from `dj.Computed`? The Computed data tier is the third major data tier in DataJoint. Tables in this data tier compute their **entries based on entries in other tables**. Computed tables are represented as red circles in the ERD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like imported tables computed tables also make use of the `populate` and `make` methods to create their entries. Let's go ahead and implement the `make` method for our `ActivityStatistics` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR CODE! - implement the `make` method\n",
    "\n",
    "@schema\n",
    "class ActivityStatistics(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> Neuron\n",
    "    ---\n",
    "    mean: float  # mean activity\n",
    "    stdev: float  # standard deviation of activity\n",
    "    max: float  # maximum activity\n",
    "    \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "        # 1. Fetch activity\n",
    "        \n",
    "        # 2. Compute mean, standard deviation and maximum\n",
    "        \n",
    "        # 3. Insert computed values\n",
    "        \n",
    "        # 4. Print message indicating for which mouse/session combination statistics were computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can populate the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActivityStatistics().populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActivityStatistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! You have computed statistics for the activity of each neuron!\n",
    "\n",
    "## Spike detection\n",
    "\n",
    "Although raw neuronal activity traces can be quite interesting, nothing is as exciting as spikes! Let's try to figure out how we could detect spikes from the traces by first visualizing them. As is tradition we start by fetching the primary keys of all our neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = Neuron.fetch(\"KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fetch the neuronal activity traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = (Neuron & keys).fetch(\"activity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we visualize the fetched traces with Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(activities), figsize=(16, 4))\n",
    "for activity, ax in zip(activities, axes.ravel()):\n",
    "    ax.plot(activity)\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Activity\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice but let's focus on one trace instead for now so that we can get a better view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = (Neuron & key).fetch1(\"activity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(activity)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Activity\")\n",
    "plt.xlim(0, 300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this we might be able to use a threshold to detect the spikes. A threshold of 0.5 may be a good starting value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "above_threshold = (activity > threshold).astype(int)\n",
    "\n",
    "plt.plot(activity)\n",
    "plt.plot(above_threshold)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.xlim(0, 300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the time points at which the activity crossed the threshold, i.e. find the transitions between adjacent time bins where `above_threshold` goes from 0 (`False`) to 1 (`True`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rising = (np.diff(above_threshold) > 0).astype(int)  # Find rising edge of crossing the threshold\n",
    "spikes = np.hstack((0, rising))  # Prepend 0 to account for shortening due to np.diff\n",
    "\n",
    "plt.plot(activity)\n",
    "plt.plot(above_threshold)\n",
    "plt.plot(np.where(spikes > 0), 1, \"ro\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.xlim(0, 300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's also compute the spike count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = spikes.sum()\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our complete spike detection algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR CODE! - try different values of threshold!\n",
    "\n",
    "threshold =     # Enter different threshold values here\n",
    "\n",
    "above_threshold = (activity > threshold).astype(int)\n",
    "\n",
    "rising = (np.diff(above_threshold) > 0).astype(int)\n",
    "spikes = np.hstack((0, rising))\n",
    "\n",
    "count = spikes.sum()\n",
    "\n",
    "plt.plot(activity)\n",
    "plt.plot(above_threshold)\n",
    "plt.plot(np.where(spikes > 0), 1, \"ro\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.title(f\"Total spike count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the exact spikes detected by the algorithm varies with the chosen `threshold` value. Therefore we might want to try different `threshold` values to see what works best. This means that the `threshold` value ought to be a parameter of the algorithm that we should be able to vary easily.\n",
    "\n",
    "In other words we want to be able to detect spikes for a **combination** of `Neuron` and `threshold`. Therefore we ought to store the different `threshold` values (and potentially any other parameters) in a separate `Lookup` table.\n",
    "\n",
    "## Parameter Lookup table\n",
    "\n",
    "Let's define the `SpikeDetectionParameters` table to hold sets of parameters for our spike detection algorithm. The Python class corresponding to this new table will inherit from `dj.Lookup`. As you might have already guessed Lookup is another data tier in DataJoint. Tables in this tier work similar to tables in the Manual data tier but Lookup tables are generally expected to contain small bits of information that is not specific to a particular experiment and fairly persistent. Our `threshold` parameter certainly falls within that description.\n",
    "\n",
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class SpikeDetectionParameters(dj.Lookup):\n",
    "    definition = \"\"\"\n",
    "    sdp_id: int  # unique id for spike detection parameter set\n",
    "    ---\n",
    "    threshold: float  # threshold for spike detection\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookup tables are depicted by gray boxes in the ERD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Spikes table\n",
    "\n",
    "Now we can define the `Spikes` table in which each entry will be a set of spikes from a particular neuron detected with a particular set of spike detection parameters. In other words each entry will be determined by a **combination of a neuron and a set of spike detection parameters**.\n",
    "\n",
    "Therefore our `Spikes` table ought to depend on the `Neuron` and `SpikeDetectionParameters` tables. Furthermore we want to store the detected spikes and the spike count in each entry.\n",
    "\n",
    "The table definition looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class Spikes(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> Neuron\n",
    "    -> SpikeDetectionParameters\n",
    "    ---\n",
    "    spikes: longblob  # detected spikes\n",
    "    count: int  # total number of detected spikes\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ERD we can see that our newly defined `Spikes` table depends on both the `Neuron` table and the `SpikeDetectionParameters` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to implement a `make` method for our `Spikes` table that detects the spikes and inserts them into the table. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class Spikes(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> Neuron\n",
    "    -> SpikeDetectionParameter\n",
    "    ---\n",
    "    spikes: longblob  # detected spikes\n",
    "    count: int  # total number of detected spikes\n",
    "    \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "        print(\"Populating for: \", key)\n",
    "\n",
    "        activity = (Neuron & key).fetch1(\"activity\")\n",
    "        threshold = (SpikeDetectionParameters & key).fetch1(\"threshold\")\n",
    "\n",
    "        above_threshold = (activity > threshold).astype(int)\n",
    "        rising = (np.diff(above_threshold) > 0).astype(int)\n",
    "        spikes = np.hstack((0, rising))\n",
    "\n",
    "        count = spikes.sum()\n",
    "        print(f\"Detected {count} spikes!\\n\")\n",
    "\n",
    "        key[\"spikes\"] = spikes\n",
    "        key[\"count\"] = count\n",
    "        self.insert1(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of the `make` method is basically identical to our earlier spike detection algorithm except that we now fetch the value of the `threshold` parameter from our `SpikeDetectionParameters` table.\n",
    "\n",
    "As expected the `Spikes` table inherits the primary key attributes from both `Neuron` (`mouse_id`, `session_date`) and `SpikeDetectionParameters` (`sdp_id`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spikes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populating the Spikes table\n",
    "\n",
    "We are now ready to populate the `Spikes` table. DataJoint will call its `make` method for each valid combination of the parent tables - `Neuron` and `SpikeDetectionParameters`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR CODE - populate the Spikes table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, weird. Nothing happened! Why might that be the case?\n",
    "\n",
    "Looking at `SpikeDetectionParameters` reveals the issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeDetectionParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is right! We still need to add a parameter set to this table. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeDetectionParameters.insert1((0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeDetectionParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing can go wrong now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR CODE! - populate the Spikes table for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spikes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally! We are now detecting spikes using DataJoint!\n",
    "\n",
    "## Trying out other parameter values\n",
    "\n",
    "Let's try a different `threshold` value to see how that affects the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeDetectionParameters.insert1((1, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeDetectionParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR CODE! - populate the missing entries in the Spikes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spikes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see spikes detected with different parameter sets can live happily next to each other without any confusion as to what is what.\n",
    "\n",
    "## Deleting entries \"upstream\"\n",
    "\n",
    "Now let's say for some reason you decide that you want to get rid of all the spikes detected with a `threshold` value of 0.5. You could do this by restricting the `Spikes` table down to the entries that depend on the specific parameter ID (i.e. `sdp_id = 0`) and delete them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Spikes & \"sdp_id = 0\").delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this would delete the unwanted spikes it would not delete the offending parameter set in the `SpikeDetectionParameters` table. So a much easier way is to just delete the parameter set and let DataJoint cascade the deletion down to the `Spikes` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeDetectionParameters & \"sdp_id = 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(SpikeDetectionParameters & \"sdp_id = 0\").delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spikes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! In this webinar we have extended our DataJoint pipeline with a table to represent recorded neural data (`Neuron` as a Imported table), tables that perform and represent computation results (`ActivityStatistict` and `Spikes` as Computed tables) and a table to hold computation parameters (`SpikeDetectionParameters` as a Lookup table).\n",
    "\n",
    "Let's take a final look at our current schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline is still fairly simple but now it is completely capable of handling analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
